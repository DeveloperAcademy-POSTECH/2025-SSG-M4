전체적인 프로세스는 **"사람 감지 → 감지된 영역별로 유니폼 분류 → 결과 취합"**의 2단계 Vision 요청으로 이루어집니다. 먼저 이미지 전체에서 사람들을 찾고, 그 다음 각 사람의 영역만 잘라내어 미리 학습시킨 유니폼 분류 모델에 넘기는 것이 정확도를 높이는 핵심입니다.

## **단계 1: Vision 요청 설정**

먼저, 사람을 감지하는 요청과 유니폼을 분류하는 요청, 두 가지를 준비합니다. 이 요청들은 한 번만 생성해서 재사용하는 것이 효율적입니다.

```Swift
import Vision
import UIKit

class ImageClassifier {
    // 1. 유니폼 분류를 위한 VNCoreMLRequest
    // Create ML로 만든 커스텀 모델을 로드하여 Vision 요청을 생성합니다.
    private lazy var classificationRequest: VNCoreMLRequest = {
        do {
            // "BaseballUniformClassifier"는 프로젝트에 추가한 mlmodel 파일의 이름입니다.
            let model = try VNCoreMLModel(for: BaseballUniformClassifier().model)
            let request = VNCoreMLRequest(model: model) { [weak self] request, error in
                // 분류 결과 처리 (아래 단계 5에서 구현)
                self?.processClassifications(for: request, error: error)
            }
            request.imageCropAndScaleOption = .scaleFill
            return request
        } catch {
            fatalError("Failed to load Vision ML model: \(error)")
        }
    }()

    // 2. 사람 감지를 위한 VNDetectHumanRectanglesRequest
    private lazy var humanDetectionRequest = VNDetectHumanRectanglesRequest(completionHandler: self.handleHumanDetection)

    // 결과(팀 이름)를 담을 배열
    private var classificationResults: [String] = []
    // 이미지 처리가 완료되었을 때 호출될 클로저
    var onCompletion: (([String: Double]) -> Void)?

    // ... 다음 단계 구현 ...
}
```

## **단계 2: 사람 감지 실행 및 결과 처리**

사용자가 이미지를 선택하면, 먼저 **사람 감지 요청(`humanDetectionRequest`)**을 실행합니다.

```Swift
extension ImageClassifier {
    // 3. 이미지 분석 시작점
    func analyzeImage(image: UIImage) {
        guard let cgImage = image.cgImage else { return }

        // 이전 결과 초기화
        self.classificationResults = []

        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        do {
            // 사람 감지 요청 실행!
            try handler.perform([self.humanDetectionRequest])
        } catch {
            print("Failed to perform human detection: \(error.localizedDescription)")
        }
    }

    // 4. 사람 감지 완료 후 호출되는 핸들러
    private func handleHumanDetection(request: VNRequest, error: Error?) {
        guard let observations = request.results as? [VNHumanObservation] else {
            self.onCompletion?([:]) // 감지된 사람 없음
            return
        }

        // 감지된 각 사람에 대해 유니폼 분류 실행
        for observation in observations {
            // 감지된 사람의 영역(boundingBox)을 기반으로 새로운 Vision 요청을 만듭니다.
            let croppedImageRequestHandler = VNImageRequestHandler(
                cgImage: request.cgImage!, // 원본 이미지에서
                regionOfInterest: observation.boundingBox, // 사람 영역만 잘라내어
                options: [:]
            )

            do {
                // 잘라낸 이미지에 대해 유니폼 분류 요청 실행!
                try croppedImageRequestHandler.perform([self.classificationRequest])
            } catch {
                print("Failed to perform classification on cropped image: \(error.localizedDescription)")
            }
        }

        // 모든 사람에 대한 분류가 끝나면 최종 결과 집계 (아래 단계 6에서 구현)
        aggregateAndFinalizeResults()
    }
}
```

- **핵심**: `handleHumanDetection` 함수에서 감지된 각 사람(`VNHumanObservation`)의 `boundingBox`를 이용해 `regionOfInterest` 옵션을 설정하고, 그 영역에 대해서만 `classificationRequest`를 실행하는 것이 중요합니다.
    

## **단계 3: 유니폼 분류 결과 처리 및 최종 집계**

각 사람 영역에 대해 유니폼 분류가 완료되면, 그 결과를 취합하여 최종 비율을 계산합니다.

``` Swift
extension ImageClassifier {
    // 5. 유니폼 분류 완료 후 호출되는 핸들러
    private func processClassifications(for request: VNRequest, error: Error?) {
        guard let classifications = request.results as? [VNClassificationObservation] else { return }

        // 가장 신뢰도 높은 결과 하나만 가져옵니다.
        if let topClassification = classifications.first {
            // 신뢰도가 일정 수준(예: 50%) 이상일 때만 결과에 추가
            if topClassification.confidence > 0.5 {
                self.classificationResults.append(topClassification.identifier)
            }
        }
    }

    // 6. 모든 결과 집계 및 비율 계산
    private func aggregateAndFinalizeResults() {
        // 모든 비동기 분류 작업이 끝났다고 가정. (실제 구현에서는 DispatchGroup 사용 권장)
        // 여기서는 간단히 모든 observation 루프가 끝난 시점에 호출

        let teamCounts = classificationResults.reduce(into: [:]) { counts, team in
            counts[team, default: 0] += 1
        }

        let totalCount = classificationResults.count
        guard totalCount > 0 else {
            self.onCompletion?([:])
            return
        }

        // 각 팀의 비율 계산
        let teamRatios = teamCounts.mapValues { Double($0) / Double(totalCount) * 100.0 }
        
        // 최종 결과를 클로저를 통해 전달
        DispatchQueue.main.async {
            self.onCompletion?(teamRatios)
        }
    }
}
```

#### **사용 예시 (UIViewController)**

```Swift
let imageClassifier = ImageClassifier()
let userSelectedImage: UIImage = // 사용자가 갤러리에서 선택한 이미지

imageClassifier.onCompletion = { results in
    // 예: "DoosanBears": 45.5, "LGTwins": 30.0
    print("팀별 응원 비율:", results)
    // 이 결과를 UI에 업데이트
}

imageClassifier.analyzeImage(image: userSelectedImage)
````



UIKit -> SwiftUI 버전
### ## 단계 1: Vision 요청 및 결과 게시 설정 (`ObservableObject`)

`UIKit`의 `onCompletion` 클로저 방식 대신, SwiftUI와 잘 동작하도록 `Combine` 프레임워크를 이용해 결과를 외부에 알리는(`@Published`) 클래스를 준비합니다.

``` swift
import Vision
import UIKit
import Combine // 1. SwiftUI와 연동하기 위해 Combine 프레임워크를 import 합니다.

// 2. 클래스가 자신의 변경사항을 외부에 알릴 수 있도록 ObservableObject 프로토콜을 채택합니다.
class ImageClassifier: ObservableObject {
    // 3. 이 프로퍼티의 값이 변경되면, 이 객체를 구독하는 모든 SwiftUI 뷰가 자동으로 업데이트됩니다.
    @Published var teamRatios: [String: Double] = [:]

    // --- Vision 요청(classificationRequest, humanDetectionRequest) 설정은 이전과 동일 ---
    private lazy var classificationRequest: VNCoreMLRequest = {
        do {
            let model = try VNCoreMLModel(for: BaseballUniformClassifier().model)
            let request = VNCoreMLRequest(model: model) { [weak self] request, error in
                self?.processClassifications(for: request, error: error)
            }
            request.imageCropAndScaleOption = .scaleFill
            return request
        } catch { fatalError("모델 로드 실패: \(error)") }
    }()

    private lazy var humanDetectionRequest = VNDetectHumanRectanglesRequest(completionHandler: self.handleHumanDetection)
    private var classificationResults: [String] = []

    // --- analyzeImage, handleHumanDetection, processClassifications 메서드는 이전과 동일 ---
    func analyzeImage(image: UIImage) {
        self.classificationResults = []
        guard let cgImage = image.cgImage else { return }
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        do {
            try handler.perform([self.humanDetectionRequest])
        } catch {
            print("사람 감지 실패: \(error.localizedDescription)")
        }
    }
    
    private func handleHumanDetection(request: VNRequest, error: Error?) { /* ... 이전과 동일 ... */ }
    private func processClassifications(for request: VNRequest, error: Error?) { /* ... 이전과 동일 ... */ }

    // --- 최종 결과 집계 방식을 @Published 프로퍼티 업데이트로 변경 ---
    private func aggregateAndFinalizeResults() {
        // ... 중간 집계 로직은 동일 ...
        let teamCounts = classificationResults.reduce(into: [:]) { $0[$1, default: 0] += 1 }
        let totalCount = classificationResults.count
        guard totalCount > 0 else {
            DispatchQueue.main.async { self.teamRatios = [:] }
            return
        }
        let finalRatios = teamCounts.mapValues { Double($0) / Double(totalCount) * 100.0 }
        
        // 4. 최종 결과를 @Published 프로퍼티에 할당합니다.
        DispatchQueue.main.async {
            self.teamRatios = finalRatios
        }
    }
}
```

---

### ## 단계 2: SwiftUI 뷰 생성 및 분석 실행

SwiftUI 뷰를 만들고, 사용자가 이미지를 선택했을 때 1단계에서 만든 `ImageClassifier`의 분석 기능을 실행하도록 연결합니다.

```Swift
import SwiftUI
import PhotosUI

struct ContentView: View {
    // 1. ImageClassifier 인스턴스를 @StateObject로 선언하여 뷰의 생명주기와 연결합니다.
    @StateObject private var classifier = ImageClassifier()

    // 2. UI 상태를 관리하기 위한 @State 프로퍼티들
    @State private var selectedImage: UIImage?
    @State private var photosPickerItem: PhotosPickerItem?

    var body: some View {
        NavigationStack {
            VStack {
                // ... 이미지와 버튼 등 UI 요소들 ...
                PhotosPicker(selection: $photosPickerItem, matching: .images) {
                    Label("사진 선택", systemImage: "photo.on.rectangle")
                }
                .buttonStyle(.borderedProminent)

                // ... 결과 표시 부분 (3단계에서 구현) ...
            }
            // 3. photosPickerItem이 변경(이미지 선택)될 때마다 아래 로직을 실행합니다.
            .onChange(of: photosPickerItem) {
                Task {
                    // 선택된 이미지 데이터를 로드합니다.
                    if let data = try? await photosPickerItem?.loadTransferable(type: Data.self),
                       let image = UIImage(data: data) {
                        
                        // 4. 로드된 이미지로 UI를 업데이트하고 분석을 시작합니다.
                        self.selectedImage = image
                        classifier.analyzeImage(image: image)
                    }
                }
            }
        }
    }
}
```

---

### ## 단계 3: 분석 결과 자동 표시 (UI 업데이트)

`ImageClassifier`의 `@Published` 프로퍼티(`teamRatios`)를 구독하여, 결과가 변경될 때마다 UI가 자동으로 다시 그려지도록 합니다.

```Swift
struct ContentView: View {
    @StateObject private var classifier = ImageClassifier()
    // ... 다른 @State 프로퍼티들 ...

    var body: some View {
        NavigationStack {
            VStack {
                // ... 이미지와 버튼 등 UI 요소들 (2단계와 동일) ...

                // 1. classifier의 teamRatios 프로퍼티가 비어있지 않을 때만 리스트를 표시합니다.
                //    이 프로퍼티가 변경되면 이 UI 영역은 자동으로 새로고침됩니다.
                if !classifier.teamRatios.isEmpty {
                    List {
                        Section("분석 결과") {
                            // 2. teamRatios 딕셔너리를 순회하며 각 팀의 이름과 비율을 표시합니다.
                            ForEach(classifier.teamRatios.sorted(by: { $0.value > $1.value }), id: \.key) { team, ratio in
                                HStack {
                                    Text(team)
                                    Spacer()
                                    Text(String(format: "%.1f%%", ratio))
                                        .fontWeight(.semibold)
                                }
                            }
                        }
                    }
                }
                
                Spacer()
            }
            .padding()
            .navigationTitle("⚾️ 유니폼 분석기")
            .onChange(of: photosPickerItem) {
                // ... 이미지 선택 및 분석 실행 로직 (2단계와 동일) ...
            }
        }
    }
}
```